---
title: "Reporting of simulation results"
author: "Angela Yu"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    df_print: paged
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(here)

knitr::opts_chunk$set(echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 9,
  fig.height = 6,
  fig.path = "../results/"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

here::i_am("analysis/final_report.Rmd")
```

## ADEMP structure

Aim: Our goal is to evaluate the performance of multiple linear regression for estimation and bootstrap-t method for confidence interval construction.

Data-generating mechanism: We generate the outcome Y by
$$
Y_i=\beta_0+\beta_{treatment} X_i +\epsilon_i,
$$
where $X_i$ is generated by `rbinom(n, 1, prob = 0.5)`.
I varied the following params:
Sample size: 10, 50, 500
True $\beta_{treatment}$: 0, 0.5, 2
True $\sigma^2$: 2
Error distribution:  "normal", "heavy-tailed"
There are 18 simulation scenarios.

Estimand: Our estmand is $\beta_{treatment}$.

Methods: We evaluate the multiple linear regression model, and compare between the Wald confidence intervals, Nonparametric bootstrap percentile intervals and Nonparametric bootstrap ùë° intervals for constructing a 95% confidence interval for the estimand.

Performance measures: The performance measures include bias, coverage, the distribution of the standard error estimates for the estmand, and the computation speed of each method.

## nSim 

coverage 95%, error no more than 1%

```{r nSim}
coverage <- 0.95
se <- 0.01
nsim <- coverage * (1 - coverage)/(se)^2
nsim
```

## Report for All

```{r report-4}
library(tidyverse)
library(knitr)

summary_t    <- read.csv(here::here("results/FINAL_SUMMARY_TABLE_parallel.csv")) %>% mutate(method = "Bootstrap-t")
summary_pct  <- read.csv(here::here("results/FINAL_SUMMARY_TABLE_parallel_precentile.csv")) %>% mutate(method = "Percentile")
summary_wald <- read.csv(here::here("results/FINAL_SUMMARY_TABLE_parallel_wald.csv")) %>% mutate(method = "Wald")

all_summaries <- bind_rows(summary_t, summary_pct, summary_wald) %>%
  mutate(
    n = rep(c(10, 50, 100), times = 18),
    beta = rep(c(0, 0.5, 2), each = 3, times = 6),
    error_type = rep(c("Normal", "Heavy-tailed"), each = 9, times = 3)
  )

bias_df <- all_summaries %>% filter(method == "Wald") 

ggplot(bias_df, aes(x = factor(n), y = bias, color = factor(beta), group = beta)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  facet_wrap(~ error_type, labeller = label_both) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  labs(title = "Bias Convergence across Sample Sizes",
       x = "Sample Size (n)", y = "Bias", color = "True Beta") +
  theme_minimal()

ggplot(all_summaries, aes(x = factor(n), y = coverage_rate, color = method, group = method)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
  facet_grid(error_type ~ beta, labeller = label_both) +
  coord_cartesian(ylim = c(0.8, 1.0)) +
  labs(title = "Coverage Rate Comparison across Methods",
       subtitle = "Percentile vs Bootstrap-t vs Wald (Target = 0.95)",
       x = "Sample Size (n)", y = "Coverage Rate") +
  theme_bw()

ggplot(all_summaries, aes(x = factor(n), y = runtime_mins, fill = method)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") + # ÂèñÂπ≥ÂùáËÄóÊó∂
  facet_wrap(~ error_type) +
  labs(title = "Average Runtime per Scenario (Averaged over Beta values)",
       y = "Mean Runtime (mins)") +
  theme_minimal()

file_list <- list.files(here::here("results"), pattern = "results_scen_.*\\_parallel_wald.csv", full.names = TRUE)

all_raw_data <- file_list %>%
  map_df(~{
    df <- read.csv(.x)
    scen_num <- as.numeric(str_extract(basename(.x), "(?<=scen_)\\d+"))
    df$scenario <- scen_num
    df
  })

all_raw_data <- all_raw_data %>%
  mutate(
    n = case_when(
      scenario %in% c(1, 4, 7, 10, 13, 16) ~ 10,
      scenario %in% c(2, 5, 8, 11, 14, 17) ~ 50,
      scenario %in% c(3, 6, 9, 12, 15, 18) ~ 100
    ),
    error_type = ifelse(scenario <= 9, "Normal", "Heavy-tailed"),
    beta_true_label = paste0("beta=", true_beta)
  )

ggplot(all_raw_data, aes(x = factor(n), y = se, fill = factor(n))) +
  geom_boxplot(alpha = 0.7, outlier.size = 0.5) +
  facet_grid(error_type ~ beta_true_label, scales = "free_y") +
  labs(
    title = "Distribution of Estimated Standard Errors (se_beta)",
    x = "Sample Size (n)",
    y = expression(paste("Estimated ", se(hat(beta)))),
    fill = "Sample Size"
  ) +
  theme_bw() +
  theme(legend.position = "bottom")


```

## Summary

The simulation study evaluates the performance of statistical estimation and construction for confidence intervals across various scenarios.
Generally, as sample size increases, estimation precision improves and bias stabilizes, yet for the distribution of $se(\hat \beta)$ under heavy-tail errors is more extreme than normal cases. For the construction of confidence intervals, all three methods get very similar coverage when sample size is large.

## Questions & Answers

### How do the different methods for constructing confidence intervals compare in terms of computation time?

The average runtime generally increases as the sample size ($n$) grows for normal errors, while the average runtime for heavy-tailed errors is similar across sample size. The runtime for bootstrap-t is significantly larger than other two methods, and wald CI is the most computational efficient.

### Which method(s) provide the best coverage when $\epsilon_i \sim N(0, 2)$?

Under Normal error distributions, the best coverage (closest to the red dashed 0.95 nominal line) is achieved by using wald CI across all sample size.

### Which method(s) provide the best coverage for the heavy-tailed errors?

For Heavy-tailed errors, the coverage is generally less stable than in the normal case. The best coverage (closest to the red dashed 0.95 nominal line) is still achieved by using wald CI across all sample size.

### Other comments

Sample Size vs. Performance: As $n$ increases from 10 to 100, the estimation bias stabilizes, and the coverage of all three methods is very close to each other. However, at a small sample size ($n=10$), the performance of the three methods diverges significantly. In particular, the Percentile method exhibits severe under-coverage, with rates dropping to approximately 0.85. In contrast, both the Wald and Bootstrap-t methods remain relatively robust. I think the problem is that if your sample size is too small, the variability of the resampling in bootstrap is limited, and thus provide a smaller CI, while wald CI is based on t-distribution, which is more protective under small sample. Although for $n=50$ and $n=100$, the coverage performance of bootstrap-t matches with wald very well, the computation cost is too large.

Error Type vs. Coverage: The model is robust under Normal errors, with coverage aligning best at $n=100$. Even though estimates get better as the sample size grows, the heavy-tailed errors still cause the confidence intervals to miss the true value more often.
